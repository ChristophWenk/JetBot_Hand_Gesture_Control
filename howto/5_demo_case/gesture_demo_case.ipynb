{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Detection / Gesture Classification Demo Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a demo case for controlling the JetBot with gestures.\n",
    "It uses self-trained models for gesture detection and classification as well as the collision avoidance model of the JetBot examples by John Welsh and Chitoku Yato.\n",
    "\n",
    "## Features\n",
    "The robot has two modes: *free* and *follow*.\n",
    "\n",
    "The default is *follow* mode. In this mode the robot will follow the *palm* gesture. <br>\n",
    "It moves towards or backwards until the detected hand has the right size. \n",
    "It will rotate to the left or right until the detected hand is in the middle of the camera image. <br>\n",
    "The robot will not move into a dangerous situation and will shake itself if it should move towards the *palm* and classifies a dangerous situation at the same time. <br>\n",
    "If the robot detects the *crawl* gesture, it will switch to *free* mode.\n",
    "\n",
    "In *free* mode the robot will move around by itself. It tries to avoid collisions and turns if it classifies a dangerous situation.\n",
    "\n",
    "In both *free* and *follow* mode the robot can classify the *five* gesture.\n",
    "This will force the robot to spin around.\n",
    "\n",
    "<img src=\"images/demo_case_large.png\" width=\"75%\">\n",
    "\n",
    "## Supported gestures\n",
    "This images taken from the [LaRED](http://mclab.citi.sinica.edu.tw/dataset/lared/lared.html) dataset demonstrate the gestures that are supported by this demo case.\n",
    "<div style=\"float:left\"><img src=\"images/crawl.jpg\" width=\"80%\"><div style=\"text-align: center\">Crawl</div></div>\n",
    "<div style=\"float:left\"><img src=\"images/fist.jpg\" width=\"80%\"><div style=\"text-align: center\">Fist</div></div>\n",
    "<div style=\"float:left\"><img src=\"images/five.jpg\" width=\"80%\"><div style=\"text-align: center\">Five</div></div>\n",
    "<div style=\"float:left\"><img src=\"images/palm.jpg\" width=\"80%\"><div style=\"text-align: center\">Palm</div></div>\n",
    "<div style=\"float:left\"><img src=\"images/peace.jpg\" width=\"80%\"><div style=\"text-align: center\">Peace</div></div>\n",
    "<div style=\"clear: both\"/><br/>\n",
    "\n",
    "**Configured gesture commands** <br>\n",
    "The following gestures have been configured to fire an action event.\n",
    "- *palm*: Follow the hand with the *palm* gesture. Switch to *follow* mode.\n",
    "- *crawl*: Switch to *free* mode. Drive around freely.\n",
    "- *five*: Spin around.\n",
    "\n",
    "## Preconditions\n",
    "The models have to be placed in the correct directory.\n",
    "Make sure you have the following folder structure **inside your notebooks directory** where the other JetBot examples can be found. <br>\n",
    "This will make sure the right dependencies are available.\n",
    "<pre>\n",
    "notebooks/\n",
    "├── basic_motion/\n",
    "├── collision_avoidance/\n",
    "└── gestures/ \n",
    "    ├── images/\n",
    "    │   ├── crawl.jpg\n",
    "    │   ├── ...\n",
    "    │   └── peace.jpg\n",
    "    ├── model_alexnet/ \n",
    "    │   └── model_collision_avoidance.pth\n",
    "    ├── model_mobilenet/\n",
    "    │   ├── saved_model.pb\n",
    "    │   └── variables/\n",
    "    │       ├── variables.data-00000-of-00002\n",
    "    │       ├── variables.data-00001-of-00002\n",
    "    │       └── variables.index\n",
    "    ├── model_ssd_mobilenetV2\n",
    "    │   └── saved_model.pb \n",
    "    ├── emobot.py\n",
    "    └── gesture_demo_case.ipynb\n",
    "</pre>\n",
    "You can rename the folders but make sure to edit the paths accordingly when loading the models. The model files may not be renamed.\n",
    "\n",
    "Make sure that you have NVIDIA `tensorflow-gpu 2.0.0+nv20.1.tf2` for Jetson Nano installed. See the [installation instructions](https://docs.nvidia.com/deeplearning/frameworks/install-tf-jetson-platform/index.html) from NVIDIA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from emobot import Emobot\n",
    "\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set necessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TO_LABEL = {\n",
    "    0 :\"crawl\",\n",
    "    1 :\"fist\",\n",
    "    2 :\"five\",\n",
    "    3 :\"palm\",\n",
    "    4 :\"peace\",\n",
    "}\n",
    "\n",
    "IMAGE_HEIGHT = 300\n",
    "IMAGE_WIDTH = 300\n",
    "\n",
    "THRESHOLD_HAND_DETECTION = 0.8\n",
    "THRESHOLD_GESTURE_CLASSIFICATION = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have problems creating the camera, uncomment the next line and run the cell again. This will restart the camera daemon\n",
    "# !echo jetbot | sudo -S systemctl restart nvargus-daemon\n",
    "\n",
    "camera = Camera.instance(width=IMAGE_WIDTH, height=IMAGE_HEIGHT)\n",
    "snapshot = widgets.Image(format='bgr8', width=IMAGE_WIDTH, height=IMAGE_HEIGHT)\n",
    "\n",
    "robot = Emobot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods\n",
    "These methods use the gesture detection and classification models.\n",
    "\n",
    "Some the methods are needed for preprocessing the model input while others use the model output and transform it for usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define font for the detections and classifications on the camera output\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "color = (0,205,205)\n",
    "fontScale = 0.35\n",
    "thickness = 1\n",
    "\n",
    "# Define collision avoidance normalization function\n",
    "# This has been defined according to the collision_avoidance example by John Welsh & Chitoku Yato\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess_collision_avoidance(camera_value):\n",
    "    \"\"\" Preprocesses the camera image for inference.\n",
    "    \n",
    "    Also prepares the data on the CUDA device.\n",
    "    This has been adapted from the collision_avoidance example by John Welsh & Chitoku Yato\n",
    "    Input:\n",
    "        camera_value: The current camera frame\n",
    "    Return:\n",
    "        x: The Tensor created from the camera image\n",
    "    \"\"\"\n",
    "    \n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x\n",
    "\n",
    "def is_collsion(img):\n",
    "    \"\"\" Checks the camera image for a potentially dangerous situation.\n",
    "    \n",
    "    Sends the image to the model for inference.\n",
    "    Input:\n",
    "        img: The current camera frame\n",
    "    Returns: \n",
    "        boolean: Robot is blocked or free\n",
    "    \"\"\"\n",
    "    \n",
    "    x = preprocess_collision_avoidance(img)\n",
    "    y = model_collision(x)\n",
    "\n",
    "    # We apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    \n",
    "    if prob_blocked < 0.5:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def filter_detected_boxes(hand_detections, threshold):\n",
    "    \"\"\" Filter the detected hands according to defined threshold.\n",
    "    \n",
    "    Only bounding boxes that have been detected above the threshold will be returned.\n",
    "    Returns: \n",
    "        boxes: Bounding boxes of detected hands\n",
    "        hand_scores: Probability that the detected object is a hand\n",
    "    \"\"\"\n",
    "    boxes_raw = hand_detections['detection_boxes'].numpy()[0]\n",
    "    scores = hand_detections['detection_scores'].numpy()[0]\n",
    "    \n",
    "    boxes = []\n",
    "    hand_scores = []\n",
    "    for i, score in enumerate(scores):\n",
    "        if score > threshold:\n",
    "            box = boxes_raw[i]\n",
    "            \n",
    "            ymin = int(float(box[0])*IMAGE_HEIGHT)\n",
    "            xmin = int(float(box[1])*IMAGE_WIDTH)\n",
    "            ymax = int(float(box[2])*IMAGE_HEIGHT)\n",
    "            xmax = int(float(box[3])*IMAGE_WIDTH)\n",
    "\n",
    "            score = round(score,2)\n",
    "            hand_scores.append(score)\n",
    "            \n",
    "            boxes.append((xmin, ymin, xmax, ymax))\n",
    "            \n",
    "    return boxes, hand_scores\n",
    "\n",
    "def crop_rect(img, xmin, xmax, ymin, ymax):\n",
    "    \"\"\" Crop the the rectangular image to a square\n",
    "    \n",
    "    Input:\n",
    "        img: Image that contains the bounding box\n",
    "        xmin, xmax, ymin, ymax: Coordinates of a hand bounding box\n",
    "    Returns: \n",
    "        img: Input image cropped to a square\n",
    "    \"\"\"\n",
    "    x, y = xmin, ymin\n",
    "    w = xmax - xmin # width\n",
    "    h = ymax - ymin # height\n",
    "    \n",
    "    # Crop a square form\n",
    "    if w > h:\n",
    "        y = y - int((w-h)/2)\n",
    "        h = w    \n",
    "        # Make sure y is within picture\n",
    "        y = max(y,0)\n",
    "        y = min(y, IMAGE_HEIGHT-h)\n",
    "\n",
    "    elif h > w:\n",
    "        x = x - int((h-w)/2)\n",
    "        w = h\n",
    "        # Make sure x is within picture\n",
    "        x = max(x,0)\n",
    "        x = min(x,IMAGE_WIDTH-w)\n",
    "        \n",
    "    return img[y:y+h, x:x+w]\n",
    "\n",
    "def add_boxes_to_img(img, boxes, hand_scores, gesture_names, gesture_scores):\n",
    "    \"\"\" Draw bounding boxes on an image\n",
    "    \n",
    "    The calculated probabilities for a hand detection or a gesture classification will be added to the image.\n",
    "    The gesture names will be drawn on the image.    \n",
    "    \n",
    "    Input:\n",
    "        img: Image that contains the bounding boxes\n",
    "        boxes: The bounding boxes to draw\n",
    "        hand_scores: The probability of a detected hand\n",
    "        gesture_names: The names of classified gestures\n",
    "        gesture_scores: The probability of a classified gesture\n",
    "    Returns: \n",
    "        img: The input image with drawn bounding boxes and scores\n",
    "    \"\"\"\n",
    "    \n",
    "    for box, hand_score, gesture_name, gesture_score in zip(boxes, hand_scores, gesture_names, gesture_scores):\n",
    "        (xmin, ymin, xmax, ymax) = box\n",
    "\n",
    "        img = cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, 1)\n",
    "        img = cv2.putText(img, 'hand ' + str(hand_score), (xmin+2, ymin+10), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        if gesture_name:\n",
    "            img = cv2.putText(img, gesture_name +\" \"+ gesture_score, (xmin+2, ymax-5), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "    return img\n",
    "\n",
    "def get_prediction_gesture(img):\n",
    "    \"\"\" Predict the gesture on the input image\n",
    "    \n",
    "    Input:\n",
    "        img: Image that contains the gesture\n",
    "    Returns: \n",
    "        gesture_name: The name of the predicted gesture\n",
    "        confidence: The probability for that gesture\n",
    "    \"\"\"\n",
    "\n",
    "    img =  cv2.resize(img, (64,64))\n",
    "    img = img/255\n",
    "    img = img.astype('float32')\n",
    "    input_tensor = tf.convert_to_tensor(img)\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "    \n",
    "    predictions = model_classify(input_tensor)\n",
    "    \n",
    "    predicted_index = np.argmax(predictions, axis=1)[0]\n",
    "    confidence = round(np.max(predictions[0]), 2)\n",
    "    \n",
    "    if confidence > THRESHOLD_GESTURE_CLASSIFICATION:\n",
    "        gesture_name = INDEX_TO_LABEL[predicted_index]\n",
    "    else:\n",
    "        gesture_name = \"\"\n",
    "    \n",
    "    return gesture_name, str(confidence)\n",
    "\n",
    "def get_biggest_gesture(boxes, hand_scores, gesture_names, gesture_scores):\n",
    "    \"\"\" Calculate which gesture covers the biggest area\n",
    "    \n",
    "    This method will filter for the gesture that covers the biggest area on an image.\n",
    "    This gesture is presumably the closest to the camera and will therefore take precedence.\n",
    "    \n",
    "    Input:\n",
    "        boxes: Bounding boxes containing the predicted gestures\n",
    "        hand_scores: Probability for the detected hands for the boxes\n",
    "        gesture_names: Names of the classified gestures for the boxes\n",
    "        gesture_scores Probability for the classified gestures for the boxes\n",
    "    Returns: \n",
    "        selected_gesture: Name of the gesture that covers the biggest area\n",
    "        selected_box: Bounding box of the gesture that covers the biggest area\n",
    "    \"\"\"\n",
    "    selected_gesture = \"\"\n",
    "    selected_box = None\n",
    "    biggest_area = 0\n",
    "    for box, hand_score, gesture_name, gesture_score in zip(boxes, hand_scores, gesture_names, gesture_scores):\n",
    "        (xmin, ymin, xmax, ymax) = box\n",
    "        current_area = (xmax-xmin)*(ymax-ymin)\n",
    "        if current_area > biggest_area:\n",
    "            selected_gesture = gesture_name\n",
    "            selected_box = box\n",
    "            biggest_area = current_area\n",
    "            \n",
    "    return selected_gesture, selected_box\n",
    "\n",
    "def classify_gestures_for_boxes(boxes):\n",
    "    \"\"\" Classify the gestures for all bounding boxes \n",
    "    \n",
    "    Input:\n",
    "        boxes: Bounding boxes containing hand detections\n",
    "    Returns: \n",
    "        gesture_names: The names of the classified gestures\n",
    "        gesture_scores: The probability of the classifed gestures\n",
    "    \"\"\"\n",
    "    gesture_names = []\n",
    "    gesture_scores = []\n",
    "    \n",
    "    for box in boxes:\n",
    "        (xmin, ymin, xmax, ymax) = box\n",
    "        cropped_image = crop_rect(img, xmin, xmax, ymin, ymax)\n",
    "        gesture_name, gesture_score = get_prediction_gesture(cropped_image)\n",
    "        gesture_names.append(gesture_name)\n",
    "        gesture_scores.append(gesture_score)\n",
    "        \n",
    "    return gesture_names, gesture_scores\n",
    "\n",
    "def rotate_to_gesture(box):\n",
    "    \"\"\" Rotates the robot towards a gestures\n",
    "    \n",
    "    The robot will rotate so that the gesture is in the middle of the camera image.\n",
    "    \n",
    "    Input:\n",
    "        box: Bounding box containing a hand detection\n",
    "    \"\"\"\n",
    "    (xmin, ymin, xmax, ymax) = box\n",
    "\n",
    "    x_center = (xmax+xmin)/2\n",
    "\n",
    "\n",
    "    distance_from_center = IMAGE_WIDTH/2 - x_center\n",
    "    distance_from_center = distance_from_center/(IMAGE_WIDTH/2) # Normalize\n",
    "    \n",
    "    if distance_from_center > 0.2:\n",
    "        robot.rotate_left_by_sight(distance_from_center)\n",
    "    elif distance_from_center < -0.2:\n",
    "        robot.rotate_right_by_sight(distance_from_center*-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models\n",
    "\n",
    "Load the hand detection, gesture classificcation and collision avoidance model. <br>\n",
    "Be patient, as the loading will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "hand detection model loaded\n",
      "gesture classification model loaded\n",
      "collosion avoidance model loaded\n"
     ]
    }
   ],
   "source": [
    "# Hand detection model\n",
    "model_dir = pathlib.Path(\"model_ssd_mobilenetV2\")\n",
    "model_detect = tf.saved_model.load(str(model_dir))\n",
    "model_detect = model_detect.signatures['serving_default']\n",
    "print(\"hand detection model loaded\")\n",
    "\n",
    "# Gesture classification model\n",
    "model_dir = pathlib.Path(\"model_mobilenet\")\n",
    "model_classify = tf.saved_model.load(str(model_dir))\n",
    "print(\"gesture classification model loaded\")\n",
    "# Warmup - Preprocess dummy\n",
    "get_prediction_gesture(camera.value)\n",
    "\n",
    "# Collosion avoidance model\n",
    "model_collision = torchvision.models.alexnet(pretrained=False)\n",
    "model_collision.classifier[6] = torch.nn.Linear(model_collision.classifier[6].in_features, 2)\n",
    "model_collision.load_state_dict(torch.load('model_alexnet/model_collision_avoidance.pth'))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model_collision = model_collision.to(device)\n",
    "print(\"collosion avoidance model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution Loop\n",
    "\n",
    "This is the \"heart\" of the demo case. The general steps are:\n",
    "1. Get new camera image\n",
    "2. Check for collision\n",
    "3. Detect hands\n",
    "4. Classify gestures\n",
    "5. Select biggest gesture\n",
    "6. Act depending on gesture\n",
    "7. Output detections/classifications to user\n",
    "\n",
    "To stop the execution click *Interrupt the kernel* in the menu and run `robot.stop()`\n",
    "\n",
    "<img src=\"images/demo_case_activity.png\" width=\"100%\"><div style=\"text-align: center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870cc5e4a9d847a69d2da88d6d2f9dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='bgr8', height='300', width='300')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4fps | Selected gesture:  rawl \r"
     ]
    }
   ],
   "source": [
    "prev_time = time.time()\n",
    "display(snapshot)\n",
    "\n",
    "MODE = \"follow\" # free, follow\n",
    "\n",
    "while True:\n",
    "    # Read camera feed\n",
    "    img = camera.value\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Check for dangerous situations\n",
    "    collision_ahead = is_collsion(img)\n",
    "    if MODE == \"free\":\n",
    "        if collision_ahead:\n",
    "            robot.left(0.3)\n",
    "        else:\n",
    "            robot.forward(0.3)\n",
    "    \n",
    "    # Create input Tensor\n",
    "    input_tensor = tf.convert_to_tensor(img)\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "    \n",
    "    # Detect hands\n",
    "    hand_detections = model_detect(input_tensor)\n",
    "    boxes, hand_scores = filter_detected_boxes(hand_detections, threshold=THRESHOLD_HAND_DETECTION)\n",
    "\n",
    "    # Classify gestures\n",
    "    gesture_names, gesture_scores = classify_gestures_for_boxes(boxes)\n",
    "\n",
    "    # Select gesture that covers biggest area\n",
    "    selected_gesture, selected_box = get_biggest_gesture(boxes, hand_scores, gesture_names, gesture_scores)\n",
    "    \n",
    "        \n",
    "    # Act on the classified gesture\n",
    "    if selected_gesture == \"five\":\n",
    "        robot.rotate_left(360);\n",
    "    elif selected_gesture == \"palm\":\n",
    "        MODE = \"follow\"\n",
    "        rotate_to_gesture(selected_box)  \n",
    "        (xmin, ymin, xmax, ymax) = selected_box\n",
    "        if ymax-ymin < IMAGE_HEIGHT/2.5:\n",
    "            if collision_ahead:\n",
    "                robot.shake_it()\n",
    "            else:\n",
    "                robot.forward(0.3)\n",
    "        elif ymax-ymin > IMAGE_HEIGHT/1.7:\n",
    "            robot.backward(0.25)\n",
    "        else:\n",
    "            robot.stop()\n",
    "    elif selected_gesture == \"crawl\":\n",
    "        MODE = \"free\"\n",
    "        robot.rotate_right(180)\n",
    "    elif MODE == \"follow\":\n",
    "        robot.stop()\n",
    "        \n",
    "    # Draw boxes and labels on displayed image\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    if collision_ahead:\n",
    "        img = cv2.putText(img, \"COLLISION\", (20, 20), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "    add_boxes_to_img(img, boxes, hand_scores, gesture_names, gesture_scores)\n",
    "    snapshot.value = bytes(cv2.imencode('.jpg', img)[1]) # Same as bgr8_to_jpeg\n",
    "    \n",
    "    fps = round(1 / (time.time() - prev_time),1)\n",
    "    print(str(fps)+\"fps\", \"| Selected gesture:\",selected_gesture, \"\\r\", end=\"\")\n",
    "    prev_time = time.time()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The kernel must be interrupted otherwise this command will not work\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}